{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import PySimpleGUI as sg\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marathi Lang Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import codecs\n",
    "import mahaNLP\n",
    "# Load the spaCy Multilingual model\n",
    "from mahaNLP.preprocess import Preprocess\n",
    "nlp = spacy.load('xx_ent_wiki_sm') # data load to identify words, punctions etc...\n",
    "\n",
    "def load_text(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "def summarize_marathi_text(text, num_sentences):  # Language detection and validation\n",
    "    detected_language = detect(text)\n",
    "    if detected_language != 'mr':  # Marathi language code is 'mr'\n",
    "        print(\"Please enter text in Marathi language.\")\n",
    "        return None  # Indicate failure or handle error message display\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\u0900-\\u097F\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def perform_marathi_stemming(text):\n",
    "    doc = text.split(\".\")\n",
    "    suffixes = {\n",
    "        1: [u\"ो\", u\"े\", u\"ू\", u\"ु\", u\"ी\", u\"ि\", u\"ा\", u\"च\"],\n",
    "        2: [u\"चा\", u\"चे\", u\"ने\", u\"नी\", u\"ना\", u\"ते\", u\"ीं\", u\"तील\", u\"ात\", u\"ाँ\", u\"ां\", u\"ों\", u\"ें\", u\"तच\", u\"ता\", u\"ही\",\n",
    "            u\"ले\"],\n",
    "        3: [u\"ाचा\", u\"ाचे\", u\"तील\", u\"ानी\", u\"ाने\", u\"ाना\", u\"ाते\", u\"ाती\", u\"ाता\", u\"तीं\", u\"तून\", u\"तील\", u\"तही\", u\"तपण\",\n",
    "            u\"कडे\", u\"ातच\", u\"हून\", u\"पणे\", u\"ाही\", u\"ाले\"],\n",
    "        4: [u\"मधले\", u\"ातील\", u\"च्या\", u\"न्या\", u\"ऱ्या\", u\"ख्या\", u\"वर\", u\"साठी\", u\"ातून\", u\"कडून\", u\"मुळे\", u\"वरून\",\n",
    "            u\"ातील\", u\"नीही\", u\"ातही\", u\"ातपण\", u\"ाकडे\", u\"पाशी\", u\"ाहून\", u\"ापणे\", u\"मधला\"],\n",
    "        5: [u\"ामधले\", u\"ाच्या\", u\"ान्या\", u\"ाऱ्या\", u\"ाख्या\", u\"ावर\", u\"ासाठी\", u\"पासून\", u\"ाकडून\", u\"ामुळे\", u\"ावरून\",\n",
    "            u\"कडेही\", u\"ानीही\", u\"ापाशी\", u\"ामधला\", u\"मध्ये\"],\n",
    "        6: [u\"पर्यंत\", u\"ापासून\", u\"ाकडेही\", u\"पूर्वक\", u\"लेल्या\", u\"ामध्ये\"],\n",
    "        7: [u\"ापर्यंत\", u\"प्रमाणे\", u\"तसुद्धा\", u\"ापूर्वक\", u\"ालेल्या\"],\n",
    "        8: [u\"ाप्रमाणे\", u\"ातसुद्धा\"],\n",
    "    }\n",
    "    preprocessed_text = []\n",
    "    # print(doc)\n",
    "    for each in doc:\n",
    "        tokens = each.split(' ')\n",
    "        cleaned_tokens = []\n",
    "        for tok in tokens:\n",
    "            if '-' in tok:\n",
    "                subtokens = tok.split('-')\n",
    "                cleaned_tokens.extend(subtokens)\n",
    "            else:\n",
    "                cleaned_tokens.append(tok.strip())\n",
    "\n",
    "        stems = []\n",
    "        for word in cleaned_tokens:\n",
    "            for i in range(8, 0, -1):\n",
    "                if len(word) > i + 1:\n",
    "                    for suf in suffixes[i]:\n",
    "                        if word.endswith(suf):\n",
    "                            word = word[:-i]\n",
    "            if word:\n",
    "                stems.append(word)\n",
    "        # if stems:\n",
    "        #     preprocessed_text.append(' '.join(stems))\n",
    "        preprocessed_text.append(' '.join(stems))\n",
    "        # preprocessed_text += \".\"\n",
    "    return preprocessed_text\n",
    "\n",
    "# Function to perform lemmatization for Marathi text\n",
    "# def perform_marathi_lemmatization(text):\n",
    "#     # Tokenize the text using spaCy\n",
    "#     doc = nlp(text)\n",
    "#     lemmatized_words = []\n",
    "#     # Iterate through each token and extract the lemma\n",
    "#     for token in doc:\n",
    "#         # If the token is not a punctuation or a space\n",
    "#         if not token.is_punct and not token.is_space:\n",
    "#             # Append the lemma to the list\n",
    "#             lemmatized_words.append(token.lemma_)\n",
    "#     # Return the list of lemmatized words\n",
    "#     return lemmatized_words\n",
    "\n",
    "# def perform_marathi_lemmatization(text):\n",
    "#     doc = nlp(text)\n",
    "#     lemmatized_words = [token.lemma_ for token in doc]\n",
    "#     return lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marathi text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marathi_text = '''एका गावात एक छोटा सुंदर मुलगा होता. त्याच्या नावाचं 'राहुल' होतं. राहुलने प्रत्येक दिवस शाळेत आणि घरीचं खेळायला मनापासून वाढवलं. त्याच्या आईबाबांनी त्याला प्रत्येक वेळी सांगितलं, \"पुढचं शिका, पुढचं उभा आणि प्रत्येक क्षण आनंदाने जगा.\" \n",
    "# एका दिवशी, राहुलने अपन्या मित्रांसोबत एका आग्रहाच्या खेळाचं स्वागत केलं. त्याच्या मनात वाटत होतं, \"माझं स्वप्न याचं सफर कसं सुरू होतं आणि कसं आनंदाने संपतं, ते सर्वांसाठी आणि माझ्या आईबाबांसाठी कसं उपयोगी होतं.\"\n",
    "# त्याच्या आग्रहाच्या खेळाचं सुरू होतं आणि राहुल आणि त्याच्या मित्रांनी खूप आनंदाने खेळलं. त्यांना वेळ अद्याप अधिक मजा केला. राहुल आणि त्याच्या मित्रांनी साथीत खेळून खूप आनंदाने वेळ व्यतीत केलं.\n",
    "# खेळाच्या शेवटी, राहुलने समजलं की, सफर महत्त्वाचं असतं. त्याचं उद्दिष्टं नक्कीपणा प्राप्त करणं आणि त्याचं स्वप्न पूर्ण करणं हे महत्त्वाचं आहे. त्याने निरंतर यशस्वीपणे काम केलं आणि आपल्या स्वप्नांचं सफर पूर्ण केलं.\n",
    "# अखेर, त्याचं उद्दिष्ट साधलं आणि त्याचं स्वप्न साकार झालं. त्याचं परिश्रम आणि आत्मविश्वास ने त्याला प्रत्येकाला आणि समुदायाला प्रेरित केलं. राहुल आणि त्याचे मित्र एकत्र येऊन एकाच नात्याने सामाजिक सेवेत सहभागी झाले आणि स्वप्न साकार झाले.\n",
    "# सद्यस्थितीत, राहुलने आपलं स्वप्न पूर्ण केलं, आणि त्याने प्रत्येक क्षण आनंदाने जगा.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize in lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_sentences = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproceed_text = preprocess_text(marathi_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatized_words = perform_marathi_lemmatization(marathi_text)\n",
    "# print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The issue is occurring because the input text is not properly tokenized before performing lemmatization. In the provided code, the input text is split by periods (`'.'`) to separate sentences, which may not work correctly for all cases, especially if there are punctuation marks within the sentences.\n",
    "\n",
    "# # To fix this issue, we need to ensure proper tokenization of the input text before performing lemmatization. We can use Spacy for tokenization and then iterate through each token to extract its lemma. Here's the modified code:\n",
    "\n",
    "# # ```python\n",
    "# import spacy\n",
    "\n",
    "# # Load the spaCy Multilingual model\n",
    "# nlp = spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "# # Function to perform lemmatization for Marathi text\n",
    "# def perform_marathi_lemmatization(text):\n",
    "#     # Tokenize the text using spaCy\n",
    "#     doc = nlp(text)\n",
    "#     lemmatized_words = []\n",
    "#     # Iterate through each token and extract the lemma\n",
    "#     for token in doc:\n",
    "#         # If the token is not a punctuation or a space\n",
    "#         if not token.is_punct and not token.is_space:\n",
    "#             # Append the lemma to the list\n",
    "#             lemmatized_words.append(token.lemma_)\n",
    "#     # Return the list of lemmatized words\n",
    "#     return lemmatized_words\n",
    "\n",
    "# # Example usage:\n",
    "# marathi_text = '''एका गावात एक छोटा सुंदर मुलगा होता. त्याच्या नावाचं 'राहुल' होतं. राहुलने प्रत्येक दिवस शाळेत आणि घरीचं खेळायला मनापासून वाढवलं. त्याच्या आईबाबांनी त्याला प्रत्येक वेळी सांगितलं, \"पुढचं शिका, पुढचं उभा आणि प्रत्येक क्षण आनंदाने जगा.\" \n",
    "# एका दिवशी, राहुलने अपन्या मित्रांसोबत एका आग्रहाच्या खेळाचं स्वागत केलं. त्याच्या मनात वाटत होतं, \"माझं स्वप्न याचं सफर कसं सुरू होतं आणि कसं आनंदाने संपतं, ते सर्वांसाठी आणि माझ्या आईबाबांसाठी कसं उपयोगी होतं.\"\n",
    "# त्याच्या आग्रहाच्या खेळाचं सुरू होतं आणि राहुल आणि त्याच्या मित्रांनी खूप आनंदाने खेळलं. त्यांना वेळ अद्याप अधिक मजा केला. राहुल आणि त्याच्या मित्रांनी साथीत खेळून खूप आनंदाने वेळ व्यतीत केलं.\n",
    "# खेळाच्या शेवटी, राहुलने समजलं की, सफर महत्त्वाचं असतं. त्याचं उद्दिष्टं नक्कीपणा प्राप्त करणं आणि त्याचं स्वप्न पूर्ण करणं हे महत्त्वाचं आहे. त्याने निरंतर यशस्वीपणे काम केलं आणि आपल्या स्वप्नांचं सफर पूर्ण केलं.\n",
    "# अखेर, त्याचं उद्दिष्ट साधलं आणि त्याचं स्वप्न साकार झालं. त्याचं परिश्रम आणि आत्मविश्वास ने त्याला प्रत्येकाला आणि समुदायाला प्रेरित केलं. राहुल आणि त्याचे मित्र एकत्र येऊन एकाच नात्याने सामाजिक सेवेत सहभागी झाले आणि स्वप्न साकार झाले.\n",
    "# सद्यस्थितीत, राहुलने आपलं स्वप्न पूर्ण केलं, आणि त्याने प्रत्येक क्षण आनंदाने जगा.'''\n",
    "# lemmatized_words = perform_marathi_lemmatization(marathi_text)\n",
    "# print(lemmatized_words)\n",
    "# # ```\n",
    "\n",
    "# # This should correctly perform le\n",
    "\n",
    "# # mmatization for the provided Marathi text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform_marathi_lemmatization(preproceed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmed_text = perform_marathi_stemming(marathi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the number of bigrams in a given sentence by tokenizing the sentence into words and generating all possible pairs of adjacent words, representing bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_bigram_length(sentence):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "    \n",
    "#     # Generate bigrams\n",
    "#     bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n",
    "    \n",
    "#     # Return the number of bigrams\n",
    "#     return len(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the number of trigrams in a given sentence by tokenizing the sentence into words and generating all possible sequences of three consecutive words, representing trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_trigram_length(sentence):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "    \n",
    "#     # Generate trigrams\n",
    "#     trigrams = [(words[i], words[i + 1], words[i + 2]) for i in range(len(words) - 2)]\n",
    "    \n",
    "#     # Return the number of trigrams\n",
    "#     return len(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a sentence as input, calculates the TF-ISF value for each term in the sentence, and returns the average TF-ISF value. TF-ISF is a measure used in information retrieval to assess the importance of terms in a document relative to the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_tf_isf(sentence):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "    \n",
    "#     # Calculate term frequency\n",
    "#     term_freq = {}\n",
    "#     for word in words:\n",
    "#         term_freq[word] = term_freq.get(word, 0) + 1\n",
    "    \n",
    "#     # Calculate inverse sentence frequency\n",
    "#     inverse_sentence_freq = 1 / len(words)\n",
    "    \n",
    "#     # Calculate TF-ISF for each term\n",
    "#     tf_isf = {}\n",
    "#     for word, freq in term_freq.items():\n",
    "#         tf_isf[word] = freq * inverse_sentence_freq\n",
    "\n",
    "#     summ = 0\n",
    "#     for val in  tf_isf.values():\n",
    "#         summ += val\n",
    "#     tf_isf = summ/len(words)\n",
    "    \n",
    "#     return tf_isf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a sentence and the maximum sentence length in a document as input, calculates the ratio of the sentence's length to the maximum length, and returns this ratio as the sentence length factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_sentence_length_factor(sentence, max_sentence_length):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "    \n",
    "#     # Calculate the length of the sentence\n",
    "#     sentence_length = len(words)\n",
    "    \n",
    "#     # Calculate the sentence length factor\n",
    "#     sentence_length_factor = sentence_length / max_sentence_length\n",
    "    \n",
    "#     return sentence_length_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a sentence as input, counts the number of numeric tokens in the sentence, calculates the ratio of numeric tokens to the total number of tokens, and returns this ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_numeric_tokens_ratio(sentence):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "    \n",
    "#     # Count the number of numeric tokens\n",
    "#     numeric_count = sum(1 for word in words if word.isdigit())\n",
    "    \n",
    "#     # Calculate the ratio of numeric tokens to total tokens\n",
    "#     if len(words) > 0:\n",
    "#         numeric_tokens_ratio = numeric_count / len(words)\n",
    "#     else:\n",
    "#         numeric_tokens_ratio = 0.0\n",
    "    \n",
    "#     return numeric_tokens_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helps prioritize or weigh sentences based on their position within the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_pos_factor(total_sentences, current_pos):\n",
    "#     \"\"\"\n",
    "#     Calculate the Position Factor (POS factor) for a given sentence.\n",
    "\n",
    "#     Parameters:\n",
    "#     total_sentences (int): Total number of sentences in the document.\n",
    "#     current_pos (int): Position of the current sentence within the document.\n",
    "\n",
    "#     Returns:\n",
    "#     float: The calculated Position Factor.\n",
    "#     \"\"\"\n",
    "#     return (total_sentences - current_pos) / total_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Thematic Number helps in assessing the thematic relevance or concentration of keywords within a sentence relative to the main themes or topics of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_thematic_number(sentence, keywords):\n",
    "#     \"\"\"\n",
    "#     Calculate the Thematic Number for a given sentence.\n",
    "\n",
    "#     Parameters:\n",
    "#     sentence (str): The input sentence.\n",
    "#     keywords (list): List of keywords representing main themes or topics of the document.\n",
    "\n",
    "#     Returns:\n",
    "#     float: The calculated Thematic Number.\n",
    "#     \"\"\"\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "    \n",
    "#     # Count the number of keywords present in the sentence\n",
    "#     keyword_count = sum(1 for word in words if word in keywords)\n",
    "    \n",
    "#     # Calculate the ratio of keywords in the sentence to the total number of keywords\n",
    "#     if len(keywords) > 0:\n",
    "#         thematic_number = keyword_count / len(keywords)\n",
    "#     else:\n",
    "#         thematic_number = 0.0\n",
    "    \n",
    "#     return thematic_number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is commonly used in text analysis tasks such as clustering or classification, where the centroid serves as a representative feature vector for the entire document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_centroid(preprocessed_text):\n",
    "#     vectorizer = CountVectorizer()\n",
    "#     X = vectorizer.fit_transform(preprocessed_text)\n",
    "#     centroid = np.mean(X.toarray(), axis=0)\n",
    "#     return centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is useful when you need to convert sparse matrices (which are memory-efficient but not directly usable for certain operations) into dense matrices (which are easier to work with but may require more memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def convert_sparse_to_dense_general(sparse_matrix, num_rows, num_cols):\n",
    "#     dense_matrix = np.zeros((num_rows, num_cols), dtype=sparse_matrix.dtype)\n",
    "#     rows, cols = sparse_matrix.nonzero()\n",
    "#     data = sparse_matrix.data\n",
    "#     for row, col, value in zip(rows, cols, data):\n",
    "#         if row < num_rows and col < num_cols:\n",
    "#             dense_matrix[row, col] = value\n",
    "#     return dense_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows for the computation of how similar a sentence is to the overall theme or content represented by the centroid of a document, which can be useful for tasks such as document classification or clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_cosine_similarity(sentence, document_centroid):\n",
    "#     document_vector = np.array([document_centroid])\n",
    "#     sentence_vector = vectorizer.transform([sentence])\n",
    "#     sentence_vector = convert_sparse_to_dense_general(sentence_vector, 1, 59)\n",
    "#     cosine_sim = cosine_similarity(sentence_vector, document_vector)[0][0]\n",
    "#     return cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function facilitates the evaluation of the importance or relevance of each sentence in the context of the entire document based on various linguistic and statistical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_sentence_scores(stemmed_text, max_sentence_length):\n",
    "#     sentence_scores = {}\n",
    "\n",
    "#     for idx, sentence in enumerate(stemmed_text):\n",
    "#         try:\n",
    "#             # Calculate various factors\n",
    "#             bigram_length = calculate_bigram_length(sentence)\n",
    "#             trigram_length = calculate_trigram_length(sentence)\n",
    "#             tf_isf = calculate_tf_isf(sentence)\n",
    "#             thematic_number = calculate_thematic_number(sentence, [])\n",
    "#             sentence_length_factor = calculate_sentence_length_factor(sentence, max_sentence_length)\n",
    "#             numeric_tokens_ratio = calculate_numeric_tokens_ratio(sentence)\n",
    "#             # Calculate sentence score\n",
    "#             sentence_score = (\n",
    "#                 bigram_length\n",
    "#                 + trigram_length\n",
    "#                 + tf_isf\n",
    "#                 + thematic_number\n",
    "#                 + sentence_length_factor\n",
    "#                 + numeric_tokens_ratio\n",
    "#             )\n",
    "#         except: \n",
    "#             sentence_score = 0\n",
    "#         sentence_scores[idx] = sentence_score\n",
    "#     return sentence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function provides a concise summary of the original text by extracting the most important sentences based on their assigned scores.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_summary(original_text, stemmed_text, sentence_scores, num_sentences=5):\n",
    "#     # Sort the sentences based on scores (in descending order)\n",
    "#     sorted_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "#     # Select the top num_sentences sentences for the summary\n",
    "#     selected_sentences = sorted_sentences[:num_sentences]\n",
    "#     # Sort selected sentences based on their original order\n",
    "#     selected_sentences.sort(key=lambda x: x[0])\n",
    "#     doc = original_text.split(\".\")\n",
    "#     # Generate the summary by joining selected sentences with periods\n",
    "#     summary = '. '.join(doc[idx] for idx, _ in selected_sentences) + '.'\n",
    "#     return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By following these steps, the provided code is able to preprocess the Marathi text, compute relevant metrics, and generate a summary that captures the key information from the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess text\n",
    "# preprocessed_text = preprocess_text(marathi_text)\n",
    "# stemmed_text = preprocessed_text.split(\".\")\n",
    "        \n",
    "# # Calculate total sentences\n",
    "# total_sentences = len(stemmed_text)\n",
    "        \n",
    "# # Calculate maximum sentence length\n",
    "# max_sentence_length = max(len(sentence.split()) for sentence in stemmed_text)\n",
    "        \n",
    "# # Calculate document centroid\n",
    "# document_centroid = calculate_centroid(stemmed_text)\n",
    "        \n",
    "# # Calculate sentence scores\n",
    "# sentence_scores = calculate_sentence_scores(stemmed_text, max_sentence_length)\n",
    "        \n",
    "# # Generate summary\n",
    "# summary = generate_summary(marathi_text, stemmed_text, sentence_scores, num_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PySimpleGUI as sg\n",
    "# from langdetect import detect\n",
    "\n",
    "\n",
    "# # Define the layout of the GUI\n",
    "# layout = [\n",
    "#     [sg.Text('Enter your Marathi text:')],\n",
    "#     [sg.Multiline(key='-TEXT-', size=(100, 12))],\n",
    "#     [sg.Text('Enter the desired number of sentences for the summary:')],\n",
    "#     [sg.InputText(key='-NUM_SENTENCES-'), sg.Button('Generate Summary'), sg.Button('Reset'), sg.Button('Exit')],\n",
    "#     [sg.Multiline(size=(100, 12), key='-OUTPUT-')],\n",
    "# ]\n",
    "\n",
    "# # Create the window\n",
    "# window = sg.Window('Marathi Text Summarizer', layout, size=(700, 520))\n",
    "\n",
    "# # Define the functions needed for summarization\n",
    "# def preprocess_text(text):\n",
    "#     # Preprocessing steps\n",
    "#     return text\n",
    "\n",
    "# # def calculate_centroid(preproceed_text):\n",
    "# #     vectorizer = TfidfVectorizer()\n",
    "# #     tfidf_matrix = vectorizer.fit_transform(preproceed_text)\n",
    "# #     centroid_vector = np.mean(tfidf_matrix, axis=0)\n",
    "# #     return centroid_vector\n",
    "\n",
    "# def convert_sparse_to_dense_general(sparse_matrix, num_rows, num_cols):\n",
    "#     dense_matrix = np.zeros((num_rows, num_cols), dtype=sparse_matrix.dtype)\n",
    "#     rows, cols = sparse_matrix.nonzero()\n",
    "#     data = sparse_matrix.data\n",
    "#     for row, col, value in zip(rows, cols, data):\n",
    "#         if row < num_rows and col < num_cols:\n",
    "#             dense_matrix[row, col] = value\n",
    "#     return dense_matrix\n",
    "\n",
    "\n",
    "# def calculate_sentence_scores(stemmed_text, total_sentences,  max_sentence_length):\n",
    "#     sentence_scores = {}\n",
    "\n",
    "#     for idx, sentence in enumerate(stemmed_text):\n",
    "#         try:\n",
    "#             pos_factor = calculate_pos_factor(total_sentences, idx + 1)\n",
    "#             bigram_length = calculate_bigram_length(sentence)\n",
    "#             trigram_length = calculate_trigram_length(sentence)\n",
    "#             tf_isf = calculate_tf_isf(sentence)\n",
    "#             # cosine_similarity_score = calculate_cosine_similarity(sentence, document_centroid)\n",
    "#             thematic_number = calculate_thematic_number(sentence,[])\n",
    "#             sentence_length_factor = calculate_sentence_length_factor(sentence, max_sentence_length)\n",
    "#             numeric_tokens_ratio = calculate_numeric_tokens_ratio(sentence)\n",
    "#             sentence_score = (\n",
    "#                 pos_factor\n",
    "#                 + bigram_length\n",
    "#                 + trigram_length\n",
    "#                 + tf_isf\n",
    "#                 # + cosine_similarity_score\n",
    "#                 + thematic_number\n",
    "#                 + sentence_length_factor\n",
    "#                 + numeric_tokens_ratio\n",
    "#             )\n",
    "#         except: \n",
    "#             sentence_score = 0\n",
    "#         sentence_scores[idx] = sentence_score\n",
    "#     return sentence_scores\n",
    "\n",
    "# def generate_summary(original_text, stemmed_text, sentence_scores, num_sentences=5):\n",
    "#     # Sort the sentences based on scores (in descending order)\n",
    "#     sorted_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "#     # Select the top num_sentences sentences for the summary\n",
    "#     selected_sentences = sorted_sentences[:num_sentences]\n",
    "#     # Sort selected sentences based on their original order\n",
    "#     selected_sentences.sort(key=lambda x: x[0])\n",
    "#     doc = original_text.split(\".\")\n",
    "#     # Generate the summary by joining selected sentences\n",
    "#     summary = ' '.join(doc[idx] for idx, _ in selected_sentences)\n",
    "#     return summary\n",
    "\n",
    "\n",
    "# def calculate_bigram_length(sentence):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "#     # Generate bigrams\n",
    "#     bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n",
    "#     # Return the number of bigrams\n",
    "#     return len(bigrams)\n",
    "\n",
    "\n",
    "# def calculate_trigram_length(sentence):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "#     # Generate trigrams\n",
    "#     trigrams = [(words[i], words[i + 1], words[i + 2]) for i in range(len(words) - 2)]\n",
    "#     # Return the number of trigrams\n",
    "#     return len(trigrams)\n",
    "\n",
    "\n",
    "# def calculate_tf_isf(sentence):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "#     # Calculate term frequency\n",
    "#     term_freq = {}\n",
    "#     for word in words:\n",
    "#         term_freq[word] = term_freq.get(word, 0) + 1\n",
    "#     # Calculate inverse sentence frequency\n",
    "#     inverse_sentence_freq = 1 / len(words)\n",
    "#     # Calculate TF-ISF for each term\n",
    "#     tf_isf = {}\n",
    "#     for word, freq in term_freq.items():\n",
    "#         tf_isf[word] = freq * inverse_sentence_freq\n",
    "#     summ = 0\n",
    "#     for val in  tf_isf.values():\n",
    "#         summ += val\n",
    "#     tf_isf = summ/len(words)\n",
    "#     return tf_isf\n",
    "\n",
    "\n",
    "# def calculate_sentence_length_factor(sentence, max_sentence_length):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "#     # Calculate the length of the sentence\n",
    "#     sentence_length = len(words)\n",
    "#     # Calculate the sentence length factor\n",
    "#     sentence_length_factor = sentence_length / max_sentence_length\n",
    "#     return sentence_length_factor\n",
    "\n",
    "\n",
    "# def calculate_numeric_tokens_ratio(sentence):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "#     # Count the number of numeric tokens\n",
    "#     numeric_count = sum(1 for word in words if word.isdigit())\n",
    "#     # Calculate the ratio of numeric tokens to total tokens\n",
    "#     if len(words) > 0:\n",
    "#         numeric_tokens_ratio = numeric_count / len(words)\n",
    "#     else:\n",
    "#         numeric_tokens_ratio = 0.0\n",
    "#     return numeric_tokens_ratio\n",
    "\n",
    "\n",
    "# def calculate_pos_factor(total_sentences, current_pos):\n",
    "#     return (total_sentences - current_pos) / total_sentences\n",
    "\n",
    "\n",
    "# def calculate_thematic_number(sentence, keywords):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "#     # Count the number of keywords present in the sentence\n",
    "#     keyword_count = sum(1 for word in words if word in keywords)\n",
    "#     # Calculate the ratio of keywords in the sentence to the total number of keywords\n",
    "#     if len(keywords) > 0:\n",
    "#         thematic_number = keyword_count / len(keywords)\n",
    "#     else:\n",
    "#         thematic_number = 0.0\n",
    "#     return thematic_number\n",
    "\n",
    "\n",
    "# def calculate_centroid(preprocessed_text):\n",
    "#     vectorizer = CountVectorizer()\n",
    "#     X = vectorizer.fit_transform(preprocessed_text)\n",
    "#     centroid = np.mean(X.toarray(), axis=0)\n",
    "#     return centroid\n",
    "\n",
    "# def cosine_similarity_with_centroid_marathi(sentence, centroid, vectorizer):\n",
    "#     sentence_vector = vectorizer.transform([sentence]).toarray()\n",
    "#     similarity = cosine_similarity(sentence_vector, [centroid])\n",
    "#     print(\"similarity: \", similarity[0][0])\n",
    "#     return similarity[0][0]\n",
    "\n",
    "# def convert_sparse_to_dense_general(sparse_matrix, num_rows, num_cols):\n",
    "#     dense_matrix = np.zeros((num_rows, num_cols), dtype=sparse_matrix.dtype)\n",
    "#     rows, cols = sparse_matrix.nonzero()\n",
    "#     data = sparse_matrix.data\n",
    "#     for row, col, value in zip(rows, cols, data):\n",
    "#         if row < num_rows and col < num_cols:\n",
    "#             dense_matrix[row, col] = value\n",
    "#     return dense_matrix\n",
    "\n",
    "# def calculate_cosine_similarity(sentence, document_centroid):\n",
    "#     document_vector = np.array([document_centroid])\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     sentence_vector = vectorizer.fit_transform([sentence])\n",
    "#     sentence_vector = convert_sparse_to_dense_general(sentence_vector, 1, 59)\n",
    "#     cosine_sim = cosine_similarity(sentence_vector, document_vector)[0][0]\n",
    "#     return cosine_sim\n",
    "\n",
    "\n",
    "# # Event loop\n",
    "# while True:\n",
    "#     event, values = window.read()\n",
    "#     if event == sg.WINDOW_CLOSED or event == 'Exit':\n",
    "#         break\n",
    "#     elif event == 'Generate Summary' or (event == '\\r' and values['-TEXT-'] != ''):\n",
    "#         # Get input values\n",
    "#         marathi_text = values['-TEXT-']\n",
    "#         num_sentences = int(values['-NUM_SENTENCES-']) if values['-NUM_SENTENCES-'] else 5\n",
    "        \n",
    "#         # Perform language detection\n",
    "#         try:\n",
    "#             if detect(marathi_text) != 'mr':  # 'mr' is the ISO code for Marathi\n",
    "#                 raise ValueError('Please input Marathi text only.')\n",
    "#         except:\n",
    "#             sg.popup_error('Please input Marathi text only.')\n",
    "#             continue\n",
    "        \n",
    "#         # Preprocess text\n",
    "#         preprocessed_text = preprocess_text(marathi_text)\n",
    "#         stemmed_text = perform_marathi_stemming(marathi_text)\n",
    "        \n",
    "#         # Calculate total sentences\n",
    "#         total_sentences = len(stemmed_text)\n",
    "        \n",
    "#         # Calculate maximum sentence length\n",
    "#         max_sentence_length = max(len(sentence.split()) for sentence in stemmed_text)\n",
    "        \n",
    "#         # Calculate sentence scores\n",
    "#         sentence_scores = calculate_sentence_scores(stemmed_text, total_sentences, max_sentence_length)\n",
    "        \n",
    "#         # Generate summary\n",
    "#         summary = generate_summary(marathi_text, stemmed_text, sentence_scores, num_sentences)\n",
    "        \n",
    "#         # Update output in GUI\n",
    "#         # sg.Print(summary)  # Print in the output element\n",
    "#         window['-OUTPUT-'].update(summary)\n",
    "        \n",
    "#     elif event == 'Reset':\n",
    "#         # Clear input and output fields\n",
    "#         window['-TEXT-'].update('')\n",
    "#         window['-NUM_SENTENCES-'].update('')\n",
    "#         window['-OUTPUT-'].update('')\n",
    "\n",
    "# # Close the window\n",
    "# window.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this optimized version:\n",
    "\n",
    "Redundant operations are minimized.\n",
    "\n",
    "The vectorizer is initialized once outside the loop.\n",
    "\n",
    "Preprocessing steps are simplified.\n",
    "\n",
    "Error handling is improved.\n",
    "\n",
    "Function calls and parameter passing are streamlined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PySimpleGUI as sg\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Define the layout of the GUI\n",
    "layout = [\n",
    "    [sg.Text('Enter your Marathi text:')],\n",
    "    [sg.Multiline(key='-TEXT-', size=(100, 12))],\n",
    "    [sg.Text('Enter the desired number of sentences for the summary:(default 5)')],\n",
    "    [sg.InputText(key='-NUM_SENTENCES-'), sg.Button('Generate Summary'), sg.Button('Reset'), sg.Button('Exit')],\n",
    "    [sg.Multiline(size=(100, 12), key='-OUTPUT-')],\n",
    "]\n",
    "\n",
    "# Create the window\n",
    "window = sg.Window('Marathi Text Summarizer', layout, size=(700, 520))\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Define the functions needed for summarization\n",
    "def preprocess_text(text):\n",
    "    # Placeholder for preprocessing steps\n",
    "    return text\n",
    "\n",
    "def calculate_sentence_scores(stemmed_text, max_sentence_length):\n",
    "    sentence_scores = {}\n",
    "\n",
    "    for idx, sentence in enumerate(stemmed_text):\n",
    "        try:\n",
    "            # Calculate various factors\n",
    "            bigram_length = calculate_bigram_length(sentence)\n",
    "            trigram_length = calculate_trigram_length(sentence)\n",
    "            tf_isf = calculate_tf_isf(sentence)\n",
    "            thematic_number = calculate_thematic_number(sentence, [])\n",
    "            sentence_length_factor = calculate_sentence_length_factor(sentence, max_sentence_length)\n",
    "            numeric_tokens_ratio = calculate_numeric_tokens_ratio(sentence)\n",
    "            # Calculate sentence score\n",
    "            sentence_score = (\n",
    "                bigram_length\n",
    "                + trigram_length\n",
    "                + tf_isf\n",
    "                + thematic_number\n",
    "                + sentence_length_factor\n",
    "                + numeric_tokens_ratio\n",
    "            )\n",
    "        except: \n",
    "            sentence_score = 0\n",
    "        sentence_scores[idx] = sentence_score\n",
    "    return sentence_scores\n",
    "\n",
    "\n",
    "def generate_summary(original_text, stemmed_text, sentence_scores, num_sentences=5):\n",
    "    # Sort the sentences based on scores (in descending order)\n",
    "    sorted_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Select the top num_sentences sentences for the summary\n",
    "    selected_sentences = sorted_sentences[:num_sentences]\n",
    "    # Sort selected sentences based on their original order\n",
    "    selected_sentences.sort(key=lambda x: x[0])\n",
    "    doc = original_text.split(\".\")\n",
    "    # Generate the summary by joining selected sentences with periods\n",
    "    summary = '. '.join(doc[idx] for idx, _ in selected_sentences) + '.'\n",
    "    return summary\n",
    "\n",
    "\n",
    "def calculate_bigram_length(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = sentence.split()\n",
    "    # Generate bigrams\n",
    "    bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n",
    "    # Return the number of bigrams\n",
    "    return len(bigrams)\n",
    "\n",
    "def calculate_trigram_length(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = sentence.split()\n",
    "    # Generate trigrams\n",
    "    trigrams = [(words[i], words[i + 1], words[i + 2]) for i in range(len(words) - 2)]\n",
    "    # Return the number of trigrams\n",
    "    return len(trigrams)\n",
    "\n",
    "def calculate_tf_isf(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = sentence.split()\n",
    "    # Calculate term frequency\n",
    "    term_freq = {}\n",
    "    for word in words:\n",
    "        term_freq[word] = term_freq.get(word, 0) + 1\n",
    "    # Calculate inverse sentence frequency\n",
    "    inverse_sentence_freq = 1 / len(words)\n",
    "    # Calculate TF-ISF for each term\n",
    "    tf_isf = sum(freq * inverse_sentence_freq for freq in term_freq.values()) / len(words)\n",
    "    return tf_isf\n",
    "\n",
    "def calculate_sentence_length_factor(sentence, max_sentence_length):\n",
    "    # Tokenize the sentence into words\n",
    "    words = sentence.split()\n",
    "    # Calculate the sentence length factor\n",
    "    sentence_length_factor = len(words) / max_sentence_length\n",
    "    return sentence_length_factor\n",
    "\n",
    "def calculate_numeric_tokens_ratio(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = sentence.split()\n",
    "    # Count the number of numeric tokens\n",
    "    numeric_count = sum(1 for word in words if word.isdigit())\n",
    "    # Calculate the ratio of numeric tokens to total tokens\n",
    "    numeric_tokens_ratio = numeric_count / len(words) if words else 0.0\n",
    "    return numeric_tokens_ratio\n",
    "\n",
    "def calculate_thematic_number(sentence, keywords):\n",
    "    # Tokenize the sentence into words\n",
    "    words = sentence.split()\n",
    "    # Calculate the ratio of keywords in the sentence to the total number of keywords\n",
    "    thematic_number = sum(1 for word in words if word in keywords) / len(keywords) if keywords else 0.0\n",
    "    return thematic_number\n",
    "\n",
    "def calculate_centroid(preprocessed_text):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(preprocessed_text)\n",
    "    centroid = np.mean(X.toarray(), axis=0)\n",
    "    return centroid\n",
    "\n",
    "def convert_sparse_to_dense_general(sparse_matrix, num_rows, num_cols):\n",
    "    dense_matrix = np.zeros((num_rows, num_cols), dtype=sparse_matrix.dtype)\n",
    "    rows, cols = sparse_matrix.nonzero()\n",
    "    data = sparse_matrix.data\n",
    "    for row, col, value in zip(rows, cols, data):\n",
    "        if row < num_rows and col < num_cols:\n",
    "            dense_matrix[row, col] = value\n",
    "    return dense_matrix\n",
    "\n",
    "def calculate_cosine_similarity(sentence, document_centroid):\n",
    "    document_vector = np.array([document_centroid])\n",
    "    sentence_vector = vectorizer.transform([sentence])\n",
    "    sentence_vector = convert_sparse_to_dense_general(sentence_vector, 1, 59)\n",
    "    cosine_sim = cosine_similarity(sentence_vector, document_vector)[0][0]\n",
    "    return cosine_sim\n",
    "\n",
    "# Event loop\n",
    "while True:\n",
    "    event, values = window.read()\n",
    "    if event == sg.WINDOW_CLOSED or event == 'Exit':\n",
    "        break\n",
    "    elif event == 'Generate Summary' or (event == '\\r' and values['-TEXT-'] != ''):\n",
    "        # Get input values\n",
    "        marathi_text = values['-TEXT-']\n",
    "        num_sentences = int(values['-NUM_SENTENCES-']) if values['-NUM_SENTENCES-'] else 5\n",
    "        \n",
    "        # Perform language detection\n",
    "        try:\n",
    "            if detect(marathi_text) != 'mr':  # 'mr' is the ISO code for Marathi\n",
    "                raise ValueError('Please input Marathi text only.')\n",
    "        except:\n",
    "            sg.popup_error('Please input Marathi text only.')\n",
    "            continue\n",
    "        \n",
    "        # Preprocess text\n",
    "        preprocessed_text = preprocess_text(marathi_text)\n",
    "        stemmed_text = preprocessed_text.split(\".\")\n",
    "        \n",
    "        # Calculate total sentences\n",
    "        total_sentences = len(stemmed_text)\n",
    "        \n",
    "        # Calculate maximum sentence length\n",
    "        max_sentence_length = max(len(sentence.split()) for sentence in stemmed_text)\n",
    "        \n",
    "        # Calculate document centroid\n",
    "        document_centroid = calculate_centroid(stemmed_text)\n",
    "        \n",
    "        # Calculate sentence scores\n",
    "        sentence_scores = calculate_sentence_scores(stemmed_text, max_sentence_length)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = generate_summary(marathi_text, stemmed_text, sentence_scores, num_sentences)\n",
    "        \n",
    "        # Update output in GUI\n",
    "        window['-OUTPUT-'].update(summary)\n",
    "        \n",
    "    elif event == 'Reset':\n",
    "        # Clear input and output fields\n",
    "        window['-TEXT-'].update('')\n",
    "        window['-NUM_SENTENCES-'].update('')\n",
    "        window['-OUTPUT-'].update('')\n",
    "\n",
    "# Close the window\n",
    "window.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Txt File import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PySimpleGUI as sg\n",
    "\n",
    "# # Define the layout of the GUI\n",
    "# layout = [\n",
    "#     [sg.Text('Select Marathi text file:'), sg.FileBrowse(key='-FILE-')],\n",
    "#     [sg.Text('Or enter your Marathi text:')],\n",
    "#     [sg.Multiline(key='-TEXT-', size=(100, 12))],\n",
    "#     [sg.Text('Enter the desired number of sentences for the summary:')],\n",
    "#     [sg.InputText(key='-NUM_SENTENCES-'), sg.Button('Generate Summary'), sg.Button('Reset'), sg.Button('Exit')],\n",
    "#     [sg.Multiline(size=(100, 12), key='-OUTPUT-')],\n",
    "# ]\n",
    "\n",
    "# # Create the window\n",
    "# window = sg.Window('Marathi Text Summarizer', layout, size=(700, 520))\n",
    "\n",
    "# # Initialize the TF-IDF vectorizer\n",
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Define the functions needed for summarization\n",
    "# def preprocess_text(text):\n",
    "#     # Placeholder for preprocessing steps\n",
    "#     return text\n",
    "\n",
    "# def calculate_sentence_scores(stemmed_text, max_sentence_length):\n",
    "#     sentence_scores = {}\n",
    "\n",
    "#     for idx, sentence in enumerate(stemmed_text):\n",
    "#         try:\n",
    "#             # Calculate various factors\n",
    "#             bigram_length = calculate_bigram_length(sentence)\n",
    "#             trigram_length = calculate_trigram_length(sentence)\n",
    "#             tf_isf = calculate_tf_isf(sentence)\n",
    "#             thematic_number = calculate_thematic_number(sentence, [])\n",
    "#             sentence_length_factor = calculate_sentence_length_factor(sentence, max_sentence_length)\n",
    "#             numeric_tokens_ratio = calculate_numeric_tokens_ratio(sentence)\n",
    "#             # Calculate sentence score\n",
    "#             sentence_score = (\n",
    "#                 bigram_length\n",
    "#                 + trigram_length\n",
    "#                 + tf_isf\n",
    "#                 + thematic_number\n",
    "#                 + sentence_length_factor\n",
    "#                 + numeric_tokens_ratio\n",
    "#             )\n",
    "#         except: \n",
    "#             sentence_score = 0\n",
    "#         sentence_scores[idx] = sentence_score\n",
    "#     return sentence_scores\n",
    "\n",
    "\n",
    "# def generate_summary(original_text, stemmed_text, sentence_scores, num_sentences=5):\n",
    "#     # Sort the sentences based on scores (in descending order)\n",
    "#     sorted_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "#     # Select the top num_sentences sentences for the summary\n",
    "#     selected_sentences = sorted_sentences[:num_sentences]\n",
    "#     # Sort selected sentences based on their original order\n",
    "#     selected_sentences.sort(key=lambda x: x[0])\n",
    "#     doc = original_text.split(\".\")\n",
    "#     # Generate the summary by joining selected sentences with periods\n",
    "#     summary = '. '.join(doc[idx] for idx, _ in selected_sentences) + '.'\n",
    "#     return summary\n",
    "\n",
    "\n",
    "# def calculate_bigram_length(sentence):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "#     # Generate bigrams\n",
    "#     bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n",
    "#     # Return the number of bigrams\n",
    "#     return len(bigrams)\n",
    "\n",
    "# def calculate_trigram_length(sentence):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "#     # Generate trigrams\n",
    "#     trigrams = [(words[i], words[i + 1], words[i + 2]) for i in range(len(words) - 2)]\n",
    "#     # Return the number of trigrams\n",
    "#     return len(trigrams)\n",
    "\n",
    "# def calculate_tf_isf(sentence):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "#     # Calculate term frequency\n",
    "#     term_freq = {}\n",
    "#     for word in words:\n",
    "#         term_freq[word] = term_freq.get(word, 0) + 1\n",
    "#     # Calculate inverse sentence frequency\n",
    "#     inverse_sentence_freq = 1 / len(words)\n",
    "#     # Calculate TF-ISF for each term\n",
    "#     tf_isf = sum(freq * inverse_sentence_freq for freq in term_freq.values()) / len(words)\n",
    "#     return tf_isf\n",
    "\n",
    "# def calculate_sentence_length_factor(sentence, max_sentence_length):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "#     # Calculate the sentence length factor\n",
    "#     sentence_length_factor = len(words) / max_sentence_length\n",
    "#     return sentence_length_factor\n",
    "\n",
    "# def calculate_numeric_tokens_ratio(sentence):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "#     # Count the number of numeric tokens\n",
    "#     numeric_count = sum(1 for word in words if word.isdigit())\n",
    "#     # Calculate the ratio of numeric tokens to total tokens\n",
    "#     numeric_tokens_ratio = numeric_count / len(words) if words else 0.0\n",
    "#     return numeric_tokens_ratio\n",
    "\n",
    "# def calculate_thematic_number(sentence, keywords):\n",
    "#     # Tokenize the sentence into words\n",
    "#     words = sentence.split()\n",
    "#     # Calculate the ratio of keywords in the sentence to the total number of keywords\n",
    "#     thematic_number = sum(1 for word in words if word in keywords) / len(keywords) if keywords else 0.0\n",
    "#     return thematic_number\n",
    "\n",
    "# def calculate_centroid(preprocessed_text):\n",
    "#     vectorizer = CountVectorizer()\n",
    "#     X = vectorizer.fit_transform(preprocessed_text)\n",
    "#     centroid = np.mean(X.toarray(), axis=0)\n",
    "#     return centroid\n",
    "\n",
    "# def convert_sparse_to_dense_general(sparse_matrix, num_rows, num_cols):\n",
    "#     dense_matrix = np.zeros((num_rows, num_cols), dtype=sparse_matrix.dtype)\n",
    "#     rows, cols = sparse_matrix.nonzero()\n",
    "#     data = sparse_matrix.data\n",
    "#     for row, col, value in zip(rows, cols, data):\n",
    "#         if row < num_rows and col < num_cols:\n",
    "#             dense_matrix[row, col] = value\n",
    "#     return dense_matrix\n",
    "\n",
    "# def calculate_cosine_similarity(sentence, document_centroid):\n",
    "#     document_vector = np.array([document_centroid])\n",
    "#     sentence_vector = vectorizer.transform([sentence])\n",
    "#     sentence_vector = convert_sparse_to_dense_general(sentence_vector, 1, 59)\n",
    "#     cosine_sim = cosine_similarity(sentence_vector, document_vector)[0][0]\n",
    "#     return cosine_sim\n",
    "\n",
    "\n",
    "# # Event loop\n",
    "# while True:\n",
    "#     event, values = window.read()\n",
    "#     if event == sg.WINDOW_CLOSED or event == 'Exit':\n",
    "#         break\n",
    "#     elif event == 'Generate Summary' or (event == '\\r' and values['-TEXT-'] != ''):\n",
    "#         # Check if the text is from file or input\n",
    "#         if values['-TEXT-'] == '':\n",
    "#             # Get input values from file\n",
    "#             file_path = values['-FILE-']\n",
    "#             if os.path.exists(file_path):\n",
    "#                 with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#                     marathi_text = file.read()\n",
    "#             else:\n",
    "#                 sg.popup_error('File not found!')\n",
    "#                 continue\n",
    "#         else:\n",
    "#             # Get input values from text input\n",
    "#             marathi_text = values['-TEXT-']\n",
    "        \n",
    "#         num_sentences = int(values['-NUM_SENTENCES-']) if values['-NUM_SENTENCES-'] else 5\n",
    "        \n",
    "#         # Perform language detection\n",
    "#         try:\n",
    "#             if detect(marathi_text) != 'mr':  # 'mr' is the ISO code for Marathi\n",
    "#                 raise ValueError('Please input Marathi text only.')\n",
    "#         except:\n",
    "#             sg.popup_error('Please input Marathi text only.')\n",
    "#             continue\n",
    "        \n",
    "#         # Preprocess text\n",
    "#         preprocessed_text = preprocess_text(marathi_text)\n",
    "#         stemmed_text = preprocessed_text.split(\".\")\n",
    "        \n",
    "#         # Calculate total sentences\n",
    "#         total_sentences = len(stemmed_text)\n",
    "        \n",
    "#         # Calculate maximum sentence length\n",
    "#         max_sentence_length = max(len(sentence.split()) for sentence in stemmed_text)\n",
    "        \n",
    "#         # Calculate document centroid\n",
    "#         document_centroid = calculate_centroid(stemmed_text)\n",
    "        \n",
    "#         # Calculate sentence scores\n",
    "#         sentence_scores = calculate_sentence_scores(stemmed_text, max_sentence_length)\n",
    "        \n",
    "#         # Generate summary\n",
    "#         summary = generate_summary(marathi_text, stemmed_text, sentence_scores, num_sentences)\n",
    "        \n",
    "#         # Update output in GUI\n",
    "#         window['-OUTPUT-'].update(summary)\n",
    "        \n",
    "#     elif event == 'Reset':\n",
    "#         # Clear input and output fields\n",
    "#         window['-TEXT-'].update('')\n",
    "#         window['-NUM_SENTENCES-'].update('')\n",
    "#         window['-OUTPUT-'].update('')\n",
    "        \n",
    "\n",
    "# # Close the window\n",
    "# window.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
